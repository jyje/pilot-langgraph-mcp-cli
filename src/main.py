"""LangGraph ì±—ë´‡ CLI ì§„ì…ì """

import asyncio
import sys
import typer
from rich.console import Console
from rich.text import Text
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.live import Live
from rich.markdown import Markdown
from typing_extensions import Annotated
from dataclasses import dataclass
from enum import Enum
import json
from pathlib import Path
from loguru import logger

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from config import check_settings, get_openai_config, get_chatbot_config
from my_mcp.logging import setup_logging
from my_mcp.service import create_chatbot_service


console = Console()

# Agent í•˜ìœ„ ì»¤ë§¨ë“œ ê·¸ë£¹ ìƒì„±
agent_app = typer.Typer(help="LangGraph ì—ì´ì „íŠ¸ ê´€ë¦¬ ëª…ë ¹ì–´")



# ì¶œë ¥ í˜•ì‹ ì •ì˜
class OutputFormat(str, Enum):
    text = "text"
    json = "json"
    yaml = "yaml"

# ì „ì—­ ìƒíƒœ ì €ì¥
state = {"options": None}

@dataclass
class CommonOptions:
    verbose: bool = False
    quiet: bool = False
    output_format: OutputFormat = OutputFormat.text
    config_file: str = None



def output_result(message: str, style: str = ""):
    """ê³µí†µ ì¶œë ¥ í•¨ìˆ˜"""
    options = state["options"]
    if options and options.quiet:
        return

    if options and options.output_format == OutputFormat.json:
        data = {"message": message, "status": "success"}
        console.print(json.dumps(data, ensure_ascii=False, indent=2))
    elif options and options.output_format == OutputFormat.yaml:
        console.print(f"message: {message}")
        console.print("status: success")
    else:
        if style:
            text = Text(message, style=style)
            console.print(text)
        else:
            console.print(message)

    if options and options.verbose:
        console.print(f"[dim]ì„¤ì • íŒŒì¼: {options.config_file or 'None'}[/dim]")

app = typer.Typer(help="OpenAI API ê¸°ë°˜ LangGraph ì±—ë´‡ CLI")

# Agent í•˜ìœ„ ì»¤ë§¨ë“œ ê·¸ë£¹ì„ ë©”ì¸ ì•±ì— ì¶”ê°€
app.add_typer(agent_app, name="agent")



@agent_app.command("export")
def export_graph(
    format: Annotated[str, typer.Option("--format", "-f", help="ì¶œë ¥ í˜•ì‹ (mermaid, json)")] = "mermaid",
    output: Annotated[str, typer.Option("--output", "-o", help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ")] = None,
    ai_description: Annotated[bool, typer.Option("--ai-description", help="AIê°€ ê·¸ë˜í”„ êµ¬ì¡° ì„¤ëª…ì„ ìë™ ìƒì„±")] = False
):
    """LangGraph ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ë‚´ë³´ëƒ…ë‹ˆë‹¤."""
    
    # ì„¤ì • íŒŒì¼ í™•ì¸
    if not check_settings():
        console.print("[red]ì„¤ì • íŒŒì¼ì„ ë¨¼ì € êµ¬ì„±í•´ì£¼ì„¸ìš”.[/red]")
        return
    
    # ê¸°ë³¸ ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    if not output:
        default_dir = Path(".my-mcp")
        try:
            # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            default_dir.mkdir(exist_ok=True)
            output = str(default_dir / "diagram.md")
            console.print(f"[dim]ê¸°ë³¸ ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤: {output}[/dim]")
        except Exception as e:
            console.print(f"[red]âŒ ê¸°ë³¸ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}[/red]")
            return
    
    try:
        # ì„¤ì • ë¡œë“œ
        openai_config = get_openai_config()
        chatbot_config = get_chatbot_config()
        
        # ì±—ë´‡ ì„œë¹„ìŠ¤ ìƒì„±
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            transient=True
        ) as progress:
            task = progress.add_task("ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...", total=None)
            chatbot_service = create_chatbot_service(openai_config, chatbot_config)
            progress.update(task, completed=100)
        
        # ê·¸ë˜í”„ êµ¬ì¡° ê°€ì ¸ì˜¤ê¸°
        graph = chatbot_service.app.get_graph()
        
        # ê·¸ë˜í”„ì—ì„œ ì‹¤ì œ ë…¸ë“œì™€ ì—£ì§€ ì •ë³´ ì¶”ì¶œ
        try:
            # LangGraphì˜ ê·¸ë˜í”„ ê°ì²´ì—ì„œ ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì¶”ì¶œ
            nodes = []
            edges = []
            
            # ê·¸ë˜í”„ê°€ dict í˜•íƒœì¸ì§€ í™•ì¸
            if hasattr(graph, 'nodes') and hasattr(graph, 'edges'):
                # ë…¸ë“œ ì •ë³´ ì¶”ì¶œ
                if hasattr(graph.nodes, '__call__'):
                    nodes = list(graph.nodes())
                elif hasattr(graph.nodes, 'keys'):
                    nodes = list(graph.nodes.keys())
                else:
                    nodes = list(graph.nodes) if graph.nodes else []
                
                # ì—£ì§€ ì •ë³´ ì¶”ì¶œ
                if hasattr(graph.edges, '__call__'):
                    raw_edges = list(graph.edges())
                    # Edge ê°ì²´ì—ì„œ sourceì™€ target ì¶”ì¶œ
                    edges = []
                    for edge in raw_edges:
                        if hasattr(edge, 'source') and hasattr(edge, 'target'):
                            edges.append((edge.source, edge.target))
                        elif isinstance(edge, (list, tuple)) and len(edge) >= 2:
                            edges.append((edge[0], edge[1]))
                elif hasattr(graph.edges, 'keys'):
                    edges = [(k, v) for k, v in graph.edges.items()]
                else:
                    edges = list(graph.edges) if graph.edges else []
            
            # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì„œë¹„ìŠ¤ ê°ì²´ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
            if not nodes:
                # ì›Œí¬í”Œë¡œìš° ê°ì²´ì—ì„œ ì§ì ‘ ë…¸ë“œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                workflow = chatbot_service.workflow
                if hasattr(workflow, 'nodes'):
                    nodes = list(workflow.nodes.keys()) if hasattr(workflow.nodes, 'keys') else []
            
            # ê·¸ë˜í”„ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°œìƒ
            if not nodes or not edges:
                raise ValueError("ê·¸ë˜í”„ êµ¬ì¡° ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ê·¸ë˜í”„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            console.print(f"[red]âŒ ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}[/red]")
            return
        
        # AI ì„¤ëª… ìƒì„±
        description = None
        if ai_description:
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                transient=True
            ) as progress:
                task = progress.add_task("AIê°€ ê·¸ë˜í”„ êµ¬ì¡° ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ì¤‘...", total=None)
                description = generate_ai_description_sync(chatbot_service, nodes, edges)
                progress.update(task, completed=100)
        
        if format.lower() == "mermaid":
            try:
                # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì¤€ë¹„
                output_path = Path(output)
                output_path.parent.mkdir(parents=True, exist_ok=True)
                
                # ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ëŸ­ìœ¼ë¡œ ê°ì‹¸ê¸°
                mermaid_content = generate_mermaid_diagram(nodes, edges, description, for_console=False)
                markdown_content = f"""# LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¡°

```mermaid
{mermaid_content}
```
"""
                if description:
                    markdown_content += f"\n## ì„¤ëª…\n\n{description}\n"
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                console.print(f"[green]âœ… Mermaid ë‹¤ì´ì–´ê·¸ë¨ì´ '{output}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.[/green]")
            except Exception as e:
                console.print(f"[red]âŒ Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì‹¤íŒ¨: {e}[/red]")
                return
                
        elif format.lower() == "json":
            # JSON í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
            import json
            
            try:
                # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì¤€ë¹„
                output_path = Path(output)
                output_path.parent.mkdir(parents=True, exist_ok=True)
                
                # JSON ë°ì´í„° ìƒì„±
                node_list = [{"id": node, "type": "node", "label": node} for node in nodes]
                edge_list = [{"source": edge[0], "target": edge[1]} for edge in edges if isinstance(edge, (list, tuple)) and len(edge) >= 2]
                
                graph_data = {
                    "nodes": node_list,
                    "edges": edge_list,
                    "workflow": "LangGraph Assistant",
                    "description": description or "ì…ë ¥ ì²˜ë¦¬ â†’ ì‘ë‹µ ìƒì„± â†’ ì¶œë ¥ í¬ë§·íŒ… ì›Œí¬í”Œë¡œìš°"
                }
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(graph_data, f, ensure_ascii=False, indent=2)
                console.print(f"[green]âœ… ê·¸ë˜í”„ êµ¬ì¡°ê°€ '{output}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.[/green]")
            except Exception as e:
                console.print(f"[red]âŒ JSON í˜•ì‹ ìƒì„± ì‹¤íŒ¨: {e}[/red]")
                return
        
        else:
            console.print(f"[red]ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤: {format}[/red]")
            console.print("ì§€ì›ë˜ëŠ” í˜•ì‹: mermaid, json")
            
    except Exception as e:
        console.print(f"[red]ê·¸ë˜í”„ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}[/red]")
        logger.error(f"ê·¸ë˜í”„ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")

def generate_ai_description_sync(chatbot_service, nodes, edges) -> str:
    """AIë¥¼ ì´ìš©í•´ ê·¸ë˜í”„ êµ¬ì¡° ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤. (ë™ê¸° ë²„ì „)"""
    try:
        # ê·¸ë˜í”„ êµ¬ì¡° ì •ë³´ ì •ë¦¬
        node_info = []
        for node in nodes:
            node_info.append(f"{node}")
        
        edge_info = []
        for edge in edges:
            if isinstance(edge, (list, tuple)) and len(edge) >= 2:
                edge_info.append(f"{edge[0]} â†’ {edge[1]}")
        
        # AIì—ê²Œ ì„¤ëª… ìƒì„± ìš”ì²­
        prompt = f"""ë‹¤ìŒ LangGraph ì›Œí¬í”Œë¡œìš°ì— ëŒ€í•œ ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ì„¤ëª…ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

ë…¸ë“œ (ì²˜ë¦¬ ë‹¨ê³„):
{', '.join(node_info)}

ì—°ê²° ê´€ê³„:
{', '.join(edge_info)}

ìš”êµ¬ì‚¬í•­:
- 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…
- ì›Œí¬í”Œë¡œìš°ì˜ ì „ì²´ì ì¸ íë¦„ê³¼ ëª©ì ì„ ì„¤ëª…
- ê¸°ìˆ ì  ìš©ì–´ë³´ë‹¤ëŠ” ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„ ì‚¬ìš©
- ì„¤ëª…ë§Œ ë°˜í™˜í•˜ê³  ë‹¤ë¥¸ ë‚´ìš©ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”"""

        # ì§ì ‘ LLM í˜¸ì¶œ (ë™ê¸° ë°©ì‹)
        from langchain.schema import HumanMessage, SystemMessage
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ì›Œí¬í”Œë¡œìš° ì„¤ëª…ì„ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”."),
            HumanMessage(content=prompt)
        ]
        
        # LLM ì§ì ‘ í˜¸ì¶œ
        response = chatbot_service.llm.invoke(messages)
        description = response.content.strip()
        
        return description
        
    except Exception as e:
        logger.error(f"AI ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
        return "ì‚¬ìš©ì ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³  AIê°€ ì‘ë‹µì„ ìƒì„±í•œ í›„ ì ì ˆí•œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ëŠ” ì›Œí¬í”Œë¡œìš°ì…ë‹ˆë‹¤."

def generate_mermaid_diagram(nodes, edges, description=None, for_console=False) -> str:
    """LangGraphë¥¼ Mermaid ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ë³€í™˜"""
    try:
        # ë…¸ë“œì™€ ì—£ì§€ê°€ ëª¨ë‘ ë¹„ì–´ìˆìœ¼ë©´ ì˜¤ë¥˜
        if not nodes or not edges:
            raise ValueError("ê·¸ë˜í”„ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        mermaid_lines = ["graph TD"]
        
        # ë…¸ë“œ ì •ì˜
        for node in nodes:
            mermaid_lines.append(f'    {node}[\"{node}\"]')
        
        # ì—£ì§€ ì •ì˜ (ì›ë³¸ ì´ë¦„ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        for edge in edges:
            if isinstance(edge, (list, tuple)) and len(edge) >= 2:
                source, target = edge[0], edge[1]
                mermaid_lines.append(f"    {source} --> {target}")
        
        # ìŠ¤íƒ€ì¼ ì¶”ê°€ (ê¸€ì”¨ ê²€ì€ìƒ‰ìœ¼ë¡œ)
        mermaid_lines.extend([
            "",
            "    classDef startEnd fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#000;",
            "    classDef process fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#000;",
            "    classDef generate fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px,color:#000;",
            "    classDef format fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#000;",
            "",
            "    class __start__,__end__ startEnd",
            "    class process_input process",
            "    class generate_response generate", 
            "    class format_output format"
        ])
        
        # ì½˜ì†” ì¶œë ¥ì¼ ë•Œë§Œ ì„¤ëª… ì¶”ê°€
        if for_console and description:
            mermaid_lines.extend([
                "",
                "---",
                f"**ì„¤ëª…**: {description}"
            ])
        
        return "\n".join(mermaid_lines)
        
    except Exception as e:
        logger.error(f"Mermaid ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì‹¤íŒ¨: {e}")
        raise e  # ì˜¤ë¥˜ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œì„œ ìƒìœ„ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•¨

@app.callback()
def main_callback(
    verbose: Annotated[bool, typer.Option("--verbose", "-v", help="ìƒì„¸ ì¶œë ¥ ëª¨ë“œ")] = False,
    quiet: Annotated[bool, typer.Option("--quiet", "-q", help="ì¡°ìš©í•œ ëª¨ë“œ")] = False,
    output_format: Annotated[OutputFormat, typer.Option("--output", "-o", help="ì¶œë ¥ í˜•ì‹")] = OutputFormat.text,
    config_file: Annotated[str, typer.Option("--config", "-c", help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")] = None,
):
    """ê³µí†µ ì˜µì…˜ ì„¤ì •"""
    state["options"] = CommonOptions(
        verbose=verbose,
        quiet=quiet,
        output_format=output_format,
        config_file=config_file
    )
    
    # loguru ë¡œê¹… ì„¤ì •
    setup_logging()

@app.command()
def chat(
    question: Annotated[str, typer.Argument(help="ì§ˆë¬¸ ë‚´ìš© (ì¼íšŒì„± ëŒ€í™” ì‹œ ì‚¬ìš©)")] = None,
    once: Annotated[bool, typer.Option("--once", help="ì¼íšŒì„± ëŒ€í™” ëª¨ë“œ (í•œ ë²ˆë§Œ ì§ˆë¬¸/ë‹µë³€ í›„ ì¢…ë£Œ)")] = False,
    no_stream: Annotated[bool, typer.Option("--no-stream", help="ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ë¹„í™œì„±í™”")] = False,
    save: Annotated[str, typer.Option("--save", help="ëŒ€í™” ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥ (íŒŒì¼ëª… ì§€ì •)")] = None
):
    """ëŒ€í™”í˜• ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤."""
    
    # ì„¤ì • íŒŒì¼ í™•ì¸
    if not check_settings():
        console.print("[red]ì„¤ì • íŒŒì¼ì„ ë¨¼ì € êµ¬ì„±í•´ì£¼ì„¸ìš”.[/red]")
        return
    
    # ë§ˆí¬ë‹¤ìš´ ì €ì¥ì„ ìœ„í•œ ëŒ€í™” ê¸°ë¡
    conversation_log = []
    
    try:
        # ì„¤ì • ë¡œë“œ
        openai_config = get_openai_config()
        chatbot_config = get_chatbot_config()
        
        # ì±—ë´‡ ì„œë¹„ìŠ¤ ìƒì„±
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            transient=True
        ) as progress:
            task = progress.add_task("ì±—ë´‡ ì´ˆê¸°í™” ì¤‘...", total=None)
            chatbot_service = create_chatbot_service(openai_config, chatbot_config)
            progress.update(task, completed=100)
        
        # ì¼íšŒì„± ëŒ€í™” ëª¨ë“œ ë˜ëŠ” ì§ˆë¬¸ì´ ì œê³µëœ ê²½ìš°
        if once or question:
            # ê°„ë‹¨í•œ í™˜ì˜ ë©”ì‹œì§€ (ì¼íšŒì„±)
            console.print(f"[bold blue]ğŸ¤– {chatbot_service.get_chatbot_name()}[/bold blue]")
            console.print("[dim]ì¼íšŒì„± ëŒ€í™” ëª¨ë“œì…ë‹ˆë‹¤.[/dim]")
            console.print()
            
            # ì§ˆë¬¸ ê²°ì •
            if question:
                user_input = question
            else:
                # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
                user_input = console.input("[bold green]ì§ˆë¬¸:[/bold green] ")
            
            # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
            if not user_input.strip():
                console.print("[yellow]ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.[/yellow]")
                return
            
            # ëŒ€í™” ìƒíƒœ ì €ì¥ (ì¼íšŒì„±ì´ë¯€ë¡œ ë¹ˆ ìƒíƒœ)
            conversation_state = {"messages": []}
            
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ í™•ì¸ (ì˜µì…˜ìœ¼ë¡œ ì¬ì •ì˜)
            streaming_enabled = get_openai_config().get("streaming", True) and not no_stream
            
            ai_response = ""
            
            if streaming_enabled:
                # AI ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
                console.print("ğŸ¤– ë‹µë³€: ", end="", style="bold cyan")
                
                async def process_stream():
                    nonlocal ai_response
                    try:
                        response_started = False
                        async for chunk in chatbot_service.chat_stream(user_input, conversation_state):
                            if not response_started:
                                response_started = True
                            console.print(chunk, end="", style="white")
                            ai_response += chunk
                        # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ì¤„ ë‚˜ëˆ” ì¶”ê°€
                        console.print("\n")
                    except Exception as e:
                        console.print(f"\n[red]ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}[/red]")
                        logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
                        ai_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                
                # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                asyncio.run(process_stream())
            else:
                # ê¸°ì¡´ ë°©ì‹ (ì „ì²´ ì‘ë‹µ í•œ ë²ˆì—)
                with Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    transient=True
                ) as progress:
                    task = progress.add_task("AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘...", total=None)
                    
                    # ë¹„ë™ê¸° í˜¸ì¶œì„ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
                    ai_response = asyncio.run(chatbot_service.chat(user_input, conversation_state))
                    progress.update(task, completed=100)
                
                # AI ì‘ë‹µ í‘œì‹œ
                ai_panel = Panel(
                    ai_response,
                    title="ğŸ¤– AI",
                    border_style="cyan"
                )
                console.print(ai_panel)
                console.print()
            
            # ë§ˆí¬ë‹¤ìš´ ì €ì¥ (ì¼íšŒì„±)
            if save:
                conversation_log.append(f"**ì‚¬ìš©ì**: {user_input}\n")
                conversation_log.append(f"**AI**: {ai_response}\n")
                save_conversation_to_markdown(conversation_log, save)
            
            # ì¼íšŒì„± ëŒ€í™” ì¢…ë£Œ
            return
        
        # ê¸°ì¡´ ì—°ì† ëŒ€í™” ëª¨ë“œ
        # í™˜ì˜ ë©”ì‹œì§€ í‘œì‹œ
        welcome_panel = Panel(
            chatbot_service.get_welcome_message(),
            title=f"ğŸ¤– {chatbot_service.get_chatbot_name()}",
            border_style="blue"
        )
        console.print(welcome_panel)
        console.print("[dim]ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ë ¤ë©´ '/bye'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.[/dim]")
        if save:
            console.print(f"[dim]ëŒ€í™” ë‚´ìš©ì´ '{save}' íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤.[/dim]")
        console.print()
        
        # ëŒ€í™” ìƒíƒœ ì €ì¥
        conversation_state = {"messages": []}
        
        # ëŒ€í™” ë£¨í”„
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
                user_input = console.input("[bold green]You:[/bold green] ")
                
                # ì¢…ë£Œ ëª…ë ¹ì–´ í™•ì¸
                if user_input.strip().lower() == "/bye":
                    console.print("[yellow]ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”! ğŸ‘‹[/yellow]")
                    break
                
                # ë¹ˆ ì…ë ¥ ë¬´ì‹œ
                if not user_input.strip():
                    continue
                
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ í™•ì¸ (ì˜µì…˜ìœ¼ë¡œ ì¬ì •ì˜)
                streaming_enabled = get_openai_config().get("streaming", True) and not no_stream
                
                ai_response = ""
                
                if streaming_enabled:
                    # AI ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
                    console.print("ğŸ¤– AI: ", end="", style="bold cyan")
                    
                    async def process_stream():
                        nonlocal ai_response
                        try:
                            response_started = False
                            async for chunk in chatbot_service.chat_stream(user_input, conversation_state):
                                if not response_started:
                                    response_started = True
                                console.print(chunk, end="", style="white")
                                ai_response += chunk
                            # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ì¤„ ë‚˜ëˆ” ì¶”ê°€
                            console.print("\n")
                        except Exception as e:
                            console.print(f"\n[red]ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}[/red]")
                            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
                            ai_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                    
                    # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                    asyncio.run(process_stream())
                else:
                    # ê¸°ì¡´ ë°©ì‹ (ì „ì²´ ì‘ë‹µ í•œ ë²ˆì—)
                    with Progress(
                        SpinnerColumn(),
                        TextColumn("[progress.description]{task.description}"),
                        transient=True
                    ) as progress:
                        task = progress.add_task("AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘...", total=None)
                        
                        # ë¹„ë™ê¸° í˜¸ì¶œì„ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
                        ai_response = asyncio.run(chatbot_service.chat(user_input, conversation_state))
                        progress.update(task, completed=100)
                    
                    # AI ì‘ë‹µ í‘œì‹œ
                    ai_panel = Panel(
                        ai_response,
                        title="ğŸ¤– AI",
                        border_style="cyan"
                    )
                    console.print(ai_panel)
                    console.print()
                
                # ë§ˆí¬ë‹¤ìš´ ì €ì¥ì„ ìœ„í•œ ëŒ€í™” ê¸°ë¡
                if save:
                    conversation_log.append(f"**ì‚¬ìš©ì**: {user_input}\n")
                    conversation_log.append(f"**AI**: {ai_response}\n")
                
            except KeyboardInterrupt:
                console.print("\n[yellow]ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”! ğŸ‘‹[/yellow]")
                break
            except Exception as e:
                console.print(f"[red]ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}[/red]")
                logger.error(f"ì±„íŒ… ì˜¤ë¥˜: {e}")
        
        # ì—°ì† ëŒ€í™” ëª¨ë“œ ì¢…ë£Œ ì‹œ ë§ˆí¬ë‹¤ìš´ ì €ì¥
        if save and conversation_log:
            save_conversation_to_markdown(conversation_log, save)
                
    except Exception as e:
        console.print(f"[red]ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}[/red]")
        logger.error(f"ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


def save_conversation_to_markdown(conversation_log: list, filename: str):
    """ëŒ€í™” ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        import datetime
        
        # íŒŒì¼ëª… ì²˜ë¦¬ (.md í™•ì¥ì ì¶”ê°€)
        if not filename.endswith('.md'):
            filename += '.md'
        
        # ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ìƒì„±
        markdown_content = f"# AI ëŒ€í™” ê¸°ë¡\n\n"
        markdown_content += f"**ìƒì„±ì¼ì‹œ**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        markdown_content += "---\n\n"
        
        for entry in conversation_log:
            markdown_content += entry + "\n"
        
        # íŒŒì¼ ì €ì¥
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        console.print(f"[green]âœ… ëŒ€í™” ë‚´ìš©ì´ '{filename}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.[/green]")
        
    except Exception as e:
        console.print(f"[red]íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}[/red]")
        logger.error(f"ë§ˆí¬ë‹¤ìš´ ì €ì¥ ì‹¤íŒ¨: {e}")

@app.command()
def info():
    """LangGraph ì±—ë´‡ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    options = state["options"]
    if options and options.output_format == OutputFormat.json:
        data = {
            "name": "LangGraph ì±—ë´‡",
            "version": "0.1.0",
            "description": "OpenAI APIë¥¼ ì´ìš©í•œ LangGraph ê¸°ë°˜ ì±—ë´‡ CLI ë„êµ¬ì…ë‹ˆë‹¤."
        }
        console.print(json.dumps(data, ensure_ascii=False, indent=2))
    elif options and options.output_format == OutputFormat.yaml:
        console.print("name: LangGraph ì±—ë´‡")
        console.print("version: 0.1.0")
        console.print("description: OpenAI APIë¥¼ ì´ìš©í•œ LangGraph ê¸°ë°˜ ì±—ë´‡ CLI ë„êµ¬ì…ë‹ˆë‹¤.")
    else:
        console.print("[bold blue]ğŸ¤– LangGraph ì±—ë´‡[/bold blue]")
        console.print("ë²„ì „: 0.1.0")
        console.print("OpenAI APIë¥¼ ì´ìš©í•œ LangGraph ê¸°ë°˜ ì±—ë´‡ CLI ë„êµ¬ì…ë‹ˆë‹¤.")

    if options and options.verbose:
        console.print(f"[dim]ì„¤ì • íŒŒì¼: {options.config_file or 'None'}[/dim]")

@app.command()
def version():
    """ë²„ì „ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    message = "LangGraph ì±—ë´‡ v0.1.0"
    output_result(message)




@app.command()
def setup():
    """ì„¤ì • íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    project_root = Path(__file__).parent.parent
    settings_file = project_root / "settings.yaml"
    sample_file = project_root / "settings.sample.yaml"
    
    if settings_file.exists():
        console.print(f"[yellow]ì„¤ì • íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {settings_file}[/yellow]")
        return
    
    if not sample_file.exists():
        console.print(f"[red]í…œí”Œë¦¿ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sample_file}[/red]")
        return
    
    try:
        # í…œí”Œë¦¿ íŒŒì¼ ë³µì‚¬
        import shutil
        shutil.copy2(sample_file, settings_file)
        
        console.print(f"[green]âœ… ì„¤ì • íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {settings_file}[/green]")
        console.print("[yellow]settings.yaml íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.[/yellow]")
        
    except Exception as e:
        console.print(f"[red]ì„¤ì • íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}[/red]")

def main():
    """ë©”ì¸ ì§„ì…ì """
    app()

if __name__ == "__main__":
    main() 